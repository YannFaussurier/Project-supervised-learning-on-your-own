{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f6jpo87Qvb_N"
      },
      "source": [
        "# Project Unsupervised learning on your own\n",
        "\n",
        "A work on text classification by Yann Faussurier.\n",
        "\n",
        "this notebook contains the final model by itself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6R8qUHYvb_P"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe8TzNGAvb_P"
      },
      "source": [
        "Now because our dataset is placed in three different files, we will load them into panda datasets in order to concatenate all of them and get a unique dataframe for our data : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abVUabqQvb_Q"
      },
      "outputs": [],
      "source": [
        "df1=pd.read_csv(\"goemotions_1.csv\")\n",
        "df2=pd.read_csv(\"goemotions_2.csv\")\n",
        "df3=pd.read_csv(\"goemotions_3.csv\")\n",
        "df=pd.concat([df1,df2,df3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYSTxF8Hvb_Q"
      },
      "source": [
        "We will now define our hyperparameters based on the example that was given with sarcasm, we will set vocab_size to 1000, embedding_dim to 16.\n",
        "\n",
        "After some trial and errors, we have set max_length to 80 because it worked better.\n",
        "\n",
        "We also set training_size to 190000 instead of 200000 in order to have a bigger testing_set, the number of data being 211225 it would let about 20 000 testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfIpaixHvb_Q"
      },
      "outputs": [],
      "source": [
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_length = 80\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 190000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMcl_J86vb_Q"
      },
      "source": [
        "Now we need to define the data that we will put into our neural network, we will set the 27 sentiment as target, and the input will be the text of the reddit comment/post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df[['text',\n",
        " 'admiration',\n",
        " 'amusement',\n",
        " 'anger',\n",
        " 'annoyance',\n",
        " 'approval',\n",
        " 'caring',\n",
        " 'confusion',\n",
        " 'curiosity',\n",
        " 'desire',\n",
        " 'disappointment',\n",
        " 'disapproval',\n",
        " 'disgust',\n",
        " 'embarrassment',\n",
        " 'excitement',\n",
        " 'fear',\n",
        " 'gratitude',\n",
        " 'grief',\n",
        " 'joy',\n",
        " 'love',\n",
        " 'nervousness',\n",
        " 'optimism',\n",
        " 'pride',\n",
        " 'realization',\n",
        " 'relief',\n",
        " 'remorse',\n",
        " 'sadness',\n",
        " 'surprise',\n",
        " 'neutral']].groupby('text').sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8-XUyV3vb_Q"
      },
      "outputs": [],
      "source": [
        "sentences = df['text']\n",
        "\n",
        "labels=df[[\n",
        " 'admiration',\n",
        " 'amusement',\n",
        " 'anger',\n",
        " 'annoyance',\n",
        " 'approval',\n",
        " 'caring',\n",
        " 'confusion',\n",
        " 'curiosity',\n",
        " 'desire',\n",
        " 'disappointment',\n",
        " 'disapproval',\n",
        " 'disgust',\n",
        " 'embarrassment',\n",
        " 'excitement',\n",
        " 'fear',\n",
        " 'gratitude',\n",
        " 'grief',\n",
        " 'joy',\n",
        " 'love',\n",
        " 'nervousness',\n",
        " 'optimism',\n",
        " 'pride',\n",
        " 'realization',\n",
        " 'relief',\n",
        " 'remorse',\n",
        " 'sadness',\n",
        " 'surprise',\n",
        " 'neutral']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rOrJUyivb_R"
      },
      "source": [
        "We can now divide our data into a training and a test set using the parameters that we have defined above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfI5sd7pvb_R"
      },
      "outputs": [],
      "source": [
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq5V5OcEvb_R"
      },
      "source": [
        "In a similar way as we did with the sarcasm dataset, we will use the tokenizer to tranform the vocabulary into tokens, and fit those tokens on the sentences of our data.\n",
        "once we have done that we use the pad_sequences function to make each of our sentences the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQR4aWbPvb_R"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOWDtWaZvb_R"
      },
      "outputs": [],
      "source": [
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6xKZ6rnvb_S"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV-yCxoBvb_S"
      },
      "source": [
        "Because the fit will take a long time, we create a callback that will stop the training if the loss on the validation set doesnt change epochs by epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_zIQuPYvb_S"
      },
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length)) #The embedding layer\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150,return_sequences=True)))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(75))) #Our LSTM layer\n",
        "model.add(tf.keras.layers.Dense(28,activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=[tf.keras.metrics.Precision()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2,callbacks=[callback])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_graphs_bis(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  #plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs_bis(history, \"precision_2\")\n",
        "plot_graphs_bis(history, \"loss\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
